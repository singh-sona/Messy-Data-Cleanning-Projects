{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRILL PART 3 | DATA MINING : BUILDING PREDCITION MODEL TO DETERMNE THE DESTINATION CITY SELECTED BY USER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This github page is part of 3-series project named DRILL whose main focus is to explore and clean Airbnb dataset and finally building prediction model to predict outcomes of destination country of users. <br>\n",
    "\n",
    ">Drill part 1 can be found in [airbnb_explore.ipynb]https://github.com/singh-sona/Messy-Data-Cleanning-Projects/blob/master/airbnb_explore.ipynb)\n",
    "\n",
    ">Drill part 2 can be found in [airbnb_explore.ipynb][https://github.com/singh-sona/Messy-Data-Cleanning-Projects/blob/master/airbnb_clean.ipynb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA TRANSFORMATION: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run airbnb_clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home made One Hot Encoding function\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_com = convert_to_binary(df=df_com, column_to_convert=column)\n",
    "    df_com.drop(column, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new data related fields\n",
    "print(\"Adding new fields...\")\n",
    "df_com['day_account_created'] = df_com['date_account_created'].dt.weekday\n",
    "df_com['month_account_created'] = df_com['date_account_created'].dt.month\n",
    "df_com['quarter_account_created'] = df_com['date_account_created'].dt.quarter\n",
    "df_com['year_account_created'] = df_com['date_account_created'].dt.year\n",
    "df_com['hour_first_active'] = df_com['timestamp_first_active'].dt.hour\n",
    "df_com['day_first_active'] = df_com['timestamp_first_active'].dt.weekday\n",
    "df_com['month_first_active'] = df_com['timestamp_first_active'].dt.month\n",
    "df_com['quarter_first_active'] = df_com['timestamp_first_active'].dt.quarter\n",
    "df_com['year_first_active'] = df_com['timestamp_first_active'].dt.year\n",
    "df_com['created_less_active'] = (df_com['date_account_created'] - df_com['timestamp_first_active']).dt.days\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_com.columns:\n",
    "        df_com.drop(column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although tranformation took two steps and  changed training dataset from 14 columns to 163 columns. Data is expanded by One Hot Encoding, which is not adding more information, but simply expanding out the existing information. No external data has been added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and working on remaining file of Sessions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_csv(\"sessions.csv\",header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine primary device\n",
    "print(\"Determing primary device...\")\n",
    "sessions_device = sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "df_primary = pd.DataFrame(aggregated_lvl1.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_primary.rename(columns = {'device_type':'primary_device', 'secs_elapsed':'primary_secs'}, inplace=True)\n",
    "df_primary = convert_to_binary(df=df_primary, column_to_convert='primary_device')\n",
    "df_primary.drop('primary_device', axis=1, inplace=True)\n",
    "\n",
    "# Determine Secondary device\n",
    "print(\"Determing secondary device...\")\n",
    "remaining = aggregated_lvl1.drop(aggregated_lvl1.index[idx])\n",
    "idx = remaining.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == remaining['secs_elapsed']\n",
    "df_secondary = pd.DataFrame(remaining.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_secondary.rename(columns = {'device_type':'secondary_device', 'secs_elapsed':'secondary_secs'}, inplace=True)\n",
    "df_secondary = convert_to_binary(df=df_secondary, column_to_convert='secondary_device')\n",
    "df_secondary.drop('secondary_device', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    id_list = df[id_col].drop_duplicates()\n",
    "    \n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]\n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "    \n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count')\n",
    "    new_df = new_df.fillna(0)\n",
    "    \n",
    "    # Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    # Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "first = True\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" column...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "\n",
    "# If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "if first:\n",
    "    first = False\n",
    "    actions_data = current_data\n",
    "else:\n",
    "    actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PREVIOUS | Drill part2: [airbnb_explore.ipynb][https://github.com/singh-sona/Messy-Data-Cleanning-Projects/blob/master/airbnb_clean.ipynb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING PREDICTION MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for modelling\n",
    "df_train.set_index('id', inplace=True)\n",
    "df_train = pd.concat([df_train['country_destination'], df_com], axis=1, join='inner')\n",
    "\n",
    "id_train = df_train.index.values\n",
    "labels = df_train['country_destination']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = df_train.drop('country_destination', axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "More to come.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
